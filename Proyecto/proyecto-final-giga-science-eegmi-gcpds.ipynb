{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5175158,"sourceType":"datasetVersion","datasetId":3008205,"isSourceIdPinned":false}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Proyecto Final Señales y Sistemas 2025 -2\n\n## **Objetivo**: Implementar técnicas de representación en tiempo y frecuencia para el reconocimiento de señales de electroencefalografía (EEG) en tareas de imaginación motora (Motor Imagery)\n\n\n![eegMI](https://figures.semanticscholar.org/288a54f091264377eccc99a19079c9387d66a78f/3-Figure2-1.png)\n\nLas señales de EEG pueden ser ruidosas debido a diversas fuentes, incluidos artefactos fisiológicos e interferencias electromagnéticas. También pueden variar de persona a persona, lo que dificulta la extracción de características y la comprensión de las señales. Además, esta variabilidad, influenciada por factores genéticos y cognitivos, presenta desafíos para el desarrollo de soluciones independientes del sujeto. \n\n**Base de datos**: GiGaScience Database [https://gigadb.org/dataset/100295](https://gigadb.org/dataset/100295)\n\nVer Sección 3.1 en [Multimodal Explainability Using Class Activation Maps and Canonical Correlation for MI-EEG Deep Learning Classification](https://www.mdpi.com/2076-3417/14/23/11208)\n","metadata":{}},{"cell_type":"markdown","source":"## Instalamos las librerias necesarias\n\n## Ejercicio 1\nConsultar para qué sirven las siguientes librerías","metadata":{}},{"cell_type":"markdown","source":"# Para que sirve cada libreria\n\n## TensorFlow 2.15.0\nTensorFlow es un framework de Google diseñado para crear y entrenar modelos de **deep learning**.  \nSus principales características:\n\n- Implementa redes neuronales convolucionales (CNN), recurrentes (RNN), transformers, autoencoders, etc.\n- Usa **Keras** como API de alto nivel.\n- Permite entrenamiento en **CPU, GPU o TPU**.\n- Ideal para proyectos de clasificación, regresión, visión por computador o series temporales.\n- Tiene herramientas para **datasets**, **callbacks**, **model.save()**, etc.\n\nEn proyectos de señales fisiológicas se usa para:\n- Clasificación de EEG con CNN.\n- Modelos para detección de estados mentales.\n- Redes en tiempo real para BCI.\n\n## MNE-Python 1.6.0\n**MNE** es la librería más completa en Python para análisis de **señales neurofisiológicas**: EEG, MEG, ECoG, SEEG, fNIRS.\n\nPermite:\n\n- **Cargar múltiples formatos**: EDF, BDF, FIF, BrainVision, etc.\n- **Filtrado** (Butterworth, FIR, IIR, notch a 50/60 Hz).\n- **Re-referenciación** (promedio, mastoides, etc.)\n- **Extracción de epochs** (segmentación de trials).\n- **ICA** para eliminar artefactos (ojos, parpadeos, EMG).\n- **Visualizaciones avanzadas**: topografías, espectrogramas, PSD.\n- **Pipeline completo para BCI**.\n\nEs ampliamente usada en investigación y papers de neurociencia.\n\n\n## Braindecode 0.7\nBraindecode es una librería especializada para **deep learning aplicado a EEG** (basada en PyTorch).\n\nIncluye:\n\n- Modelos clásicos:  \n  - **DeepConvNet**  \n  - **ShallowFBCSPNet**  \n  - **EEGNet**  \n  - **TCN**, **SleepStager**, etc.\n- Compatible con **MNE** (trabaja con objetos Raw y Epochs).\n- Herramientas para:\n  - entrenar modelos\n  - crear datasets de EEG\n  - normalizar señales\n  - aplicar aumentación (data augmentation)\n\nEs ideal para proyectos de:\n- Clasificación de imaginación motora (MI)\n- Detección de eventos cerebrales\n- Sleep staging\n- BCI en general.\n\n## gcpds.databases  \nPaquete creado por el **Grupo GCPDS (Universidad Nacional de Colombia)**.\n\nSirve para:\n\n- **Descargar y cargar datasets** creados por el grupo.  \n- Organizar datos en formato estándar para ML.\n- Acceder a bases como:  \n  - `GIGA_MI_ME` → EEG para imaginación motora  \n  - (otros datasets según repositorio)\n\nFacilita el acceso a datos listos para análisis o entrenamiento.\n\n## SciPy.signal (resample, freqz, filtfilt, butter)\nEstas funciones son de la sublibrería **scipy.signal**, usada para procesamiento digital de señales:\n\n### `resample()`\n- Cambia la frecuencia de muestreo de una señal.\n- Utiliza FFT para reinterpolar la señal.\n\n### `freqz()`\n- Obtiene la **respuesta en frecuencia** de un filtro digital.\n- Permite ver cómo se comporta un filtro (ganancia, corte).\n\n### `filtfilt()`\n- Aplica un filtro en ambos sentidos → **sin desfase de fase**.\n- Fundamental en EEG para evitar atrasos del filtro.\n\n### `butter()` (renombrado como `bw`)\n- Diseña filtros Butterworth.\n- Permite crear pasa-banda, pasa-bajos, pasa-altos, etc.\n\n\n## pandas\nLibrería estándar para:\n\n- Cargar datos (`csv`, `xlsx`, `sql`)\n- Manipular tablas\n- Limpiar y organizar datos\n- Agrupar y resumir información\n\nUsada para manejar:\n- Metadata de sujetos\n- Eventos/etiquetas\n- Resultados de entrenamiento\n\n## numpy\nBase matemática de Python para:\n\n- Vectores y matrices\n- Operaciones numéricas\n- Transformaciones\n- Creación de señales sintéticas\n\nTodo el procesamiento de señales en SciPy/MNE se basa en `numpy`.\n\n\n## matplotlib.pyplot\nBiblioteca para graficar:\n\n- Señales en el tiempo\n- PSD\n- Respuesta de filtros\n- Espectrogramas\n- Curvas de entrenamiento\n\nMuy usada para visualizar EEG, filtros o resultados.\n\n\n## sklearn.base (BaseEstimator, TransformerMixin)\nEstas clases permiten crear **transformadores personalizados** compatibles con scikit-learn.\n\nSirven para:\n\n- Crear módulos de preprocesamiento (filtros, normalizadores, etc.)\n- Integrarlos en `Pipeline`\n- Usarlos con `GridSearchCV` o `RandomizedSearchCV`\n\nEjemplo típico:\n- crear un filtro EEG “a tu medida”  \n- usarlo dentro de un pipeline ML.\n","metadata":{}},{"cell_type":"code","source":"#!pip install tensorflow==2.15.0\n!pip install mne==1.6.0\n!pip install braindecode===0.7\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.databases\n!pip install pyriemann\n","metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importamos algunas librerias necesarias","metadata":{}},{"cell_type":"code","source":"from scipy.signal import resample\nfrom scipy.signal import freqz, filtfilt, resample\nfrom scipy.signal import butter as bw\nimport pandas as pd\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import tensorflow as tf\nfrom gcpds.databases import GIGA_MI_ME\nfrom sklearn.base import BaseEstimator, TransformerMixin\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Funciones necesarias para el preprocesamiento leve de los datos","metadata":{}},{"cell_type":"code","source":"def load_GIGA(db,\n              sbj,\n              eeg_ch_names,\n              new_fs,\n              fs,\n              f_bank=None,\n              vwt=None,           \n              run=None):\n\n    index_eeg_chs = db.format_channels_selectors(channels = eeg_ch_names) - 1\n\n    #tf_repr = TimeFrequencyRpr(sfreq = fs, f_bank = f_bank, vwt = vwt)\n\n    db.load_subject(sbj)\n    if run == None:\n        X, y = db.get_data(classes = ['left hand mi', 'right hand mi']) #Load MI classes, all channels {EEG}, reject bad trials, uV\n    else:\n        X, y = db.get_run(run, classes = ['left hand mi', 'right hand mi']) #Load MI classes, all channels {EEG}, reject bad trials, uV\n    X = X[:, index_eeg_chs, :] #spatial rearrangement\n    #X = np.squeeze(tf_repr.transform(X))\n    #Resampling\n    if new_fs == fs:\n        pass#print('No resampling, since new sampling rate same.')\n    else:\n        print(\"Resampling from {:f} to {:f} Hz.\".format(fs, new_fs))\n        X = resample(X, int((X.shape[-1]/fs)*new_fs), axis = -1)\n\n    return X, y\n\ndef butterworth_digital_filter(X, N, Wn, btype, fs, axis=-1, padtype=None, padlen=0, method='pad', irlen=None):\n  \"\"\"\n  Apply digital butterworth filter\n  INPUT\n  ------\n  1. X: (D array)\n    array with signals.\n  2. N: (int+)\n    The order of the filter.\n  3. Wn: (float+ or 1D array)\n    The critical frequency or frequencies. For lowpass and highpass filters, Wn is a scalar; for bandpass and bandstop filters, Wn is a length-2 vector.\n    For a Butterworth filter, this is the point at which the gain drops to 1/sqrt(2) that of the passband (the “-3 dB point”).\n    If fs is not specified, Wn units are normalized from 0 to 1, where 1 is the Nyquist frequency (Wn is thus in half cycles / sample and defined as 2*critical frequencies / fs). If fs is specified, Wn is in the same units as fs.\n  4. btype: (str) {‘lowpass’, ‘highpass’, ‘bandpass’, ‘bandstop’}\n    The type of filter\n  5. fs: (float+)\n    The sampling frequency of the digital system.\n  6. axis: (int), Default=1.\n    The axis of x to which the filter is applied.\n  7. padtype: (str) or None, {'odd', 'even', 'constant'}\n    This determines the type of extension to use for the padded signal to which the filter is applied. If padtype is None, no padding is used. The default is ‘odd’.\n  8. padlen: (int+) or None, Default=0\n    The number of elements by which to extend x at both ends of axis before applying the filter. This value must be less than x.shape[axis] - 1. padlen=0 implies no padding.\n  9. method: (str), {'pad', 'gust'}\n    Determines the method for handling the edges of the signal, either “pad” or “gust”. When method is “pad”, the signal is padded; the type of padding is determined by padtype\n    and padlen, and irlen is ignored. When method is “gust”, Gustafsson’s method is used, and padtype and padlen are ignored.\n  10. irlen: (int) or None, Default=nONE\n    When method is “gust”, irlen specifies the length of the impulse response of the filter. If irlen is None, no part of the impulse response is ignored.\n    For a long signal, specifying irlen can significantly improve the performance of the filter.\n  OUTPUT\n  ------\n  X_fil: (D array)\n    array with filtered signals.\n  \"\"\"\n  b, a = bw(N, Wn, btype, analog=False, output='ba', fs=fs)\n  return filtfilt(b, a, X, axis=axis, padtype=padtype, padlen=padlen, method=method, irlen=irlen)\n\nclass TimeFrequencyRpr(BaseEstimator, TransformerMixin):\n  \"\"\"\n  Time frequency representation of EEG signals.\n\n  Parameters\n  ----------\n    1. sfreq:  (float) Sampling frequency in Hz.\n    2. f_bank: (2D array) Filter banks Frequencies. Default=None\n    3. vwt:    (2D array) Interest time windows. Default=None\n  Methods\n  -------\n    1. fit(X, y=None)\n    2. transform(X, y=None)\n  \"\"\"\n  def __init__(self, sfreq, f_bank=None, vwt=None):\n    self.sfreq = sfreq\n    self.f_bank = f_bank\n    self.vwt = vwt\n# ------------------------------------------------------------------------------\n\n  def _validation_param(self):\n    \"\"\"\n    Validate Time-Frequency characterization parameters.\n    INPUT\n    -----\n      1. self\n    ------\n      2. None\n    \"\"\"\n    if self.sfreq <= 0:\n      raise ValueError('Non negative sampling frequency is accepted')\n\n\n    if self.f_bank is None:\n      self.flag_f_bank = False\n    elif self.f_bank.ndim != 2:\n      raise ValueError('Band frequencies have to be a 2D array')\n    else:\n      self.flag_f_bank = True\n\n    if self.vwt is None:\n      self.flag_vwt = False\n    elif self.vwt.ndim != 2:\n      raise ValueError('Time windows have to be a 2D array')\n    else:\n      self.flag_vwt = True\n\n# ------------------------------------------------------------------------------\n  def _filter_bank(self, X):\n    \"\"\"\n    Filter bank Characterization.\n    INPUT\n    -----\n      1. X: (3D array) set of EEG signals, shape (trials, channels, time_samples)\n    OUTPUT\n    ------\n      1. X_f: (4D array) set of filtered EEG signals, shape (trials, channels, time_samples, frequency_bands)\n    \"\"\"\n    X_f = np.zeros((X.shape[0], X.shape[1], X.shape[2], self.f_bank.shape[0])) #epochs, Ch, Time, bands\n    for f in np.arange(self.f_bank.shape[0]):\n      X_f[:,:,:,f] = butterworth_digital_filter(X, N=5, Wn=self.f_bank[f], btype='bandpass', fs=self.sfreq)\n    return X_f\n\n# ------------------------------------------------------------------------------\n  def _sliding_windows(self, X):\n    \"\"\"\n    Sliding Windows Characterization.\n    INPUT\n    -----\n      1. X: (3D array) set of EEG signals, shape (trials, channels, time_samples)\n    OUTPUT\n    ------\n      1. X_w: (4D array) shape (trials, channels, window_time_samples, number_of_windows)\n    \"\"\"\n    window_lenght = int(self.sfreq*self.vwt[0,1] - self.sfreq*self.vwt[0,0])\n    X_w = np.zeros((X.shape[0], X.shape[1], window_lenght, self.vwt.shape[0]))\n    for w in np.arange(self.vwt.shape[0]):\n        X_w[:,:,:,w] = X[:,:,int(self.sfreq*self.vwt[w,0]):int(self.sfreq*self.vwt[w,1])]\n    return X_w\n\n# ------------------------------------------------------------------------------\n  def fit(self, X, y=None):\n    \"\"\"\n    fit.\n    INPUT\n    -----\n      1. X: (3D array) set of EEG signals, shape (trials, channels, time_samples)\n      2. y: (1D array) target labels. Default=None\n    OUTPUT\n    ------\n      1. None\n    \"\"\"\n    pass\n\n# ------------------------------------------------------------------------------\n  def transform(self, X, y=None):\n    \"\"\"\n    Time frequency representation of EEG signals.\n    INPUT\n    -----\n      1. X: (3D array) set of EEG signals, shape (trials, channels, times)\n    OUTPUT\n    ------\n      1. X_wf: (5D array) Time-frequency representation of EEG signals, shape (trials, channels, window_time_samples, number_of_windows, frequency_bands)\n    \"\"\"\n    self._validation_param()     #Validate sfreq, f_freq, vwt\n\n    #Avoid edge effects of digital filter, 1st:fbk, 2th:vwt\n    if self.flag_f_bank:\n        X_f = self._filter_bank(X)\n    else:\n        X_f = X[:,:,:,np.newaxis]\n\n    if self.flag_vwt:\n      X_wf = []\n      for f in range(X_f.shape[3]):\n        X_wf.append(self._sliding_windows(X_f[:,:,:,f]))\n      X_wf = np.stack(X_wf, axis=-1)\n    else:\n      X_wf = X_f[:,:,:,np.newaxis,:]\n\n    return X_wf\n\n#plot eeg   \ndef plot_eeg(X,tv,ax,channels,esp=2,title=None):\n    # X in CH x Samples\n    n_canales = X.shape[0]\n\n    for ch in range(n_canales): # canales\n            xx = X[ch]\n            xx = xx - np.mean(xx)\n            xx = xx/np.max(abs(xx))\n            ax.plot(tv, xx +(ch * esp), label=channels[ch])  # Desplazamos cada canal para visualización\n    ax.set_yticks(range(0, esp * n_canales, esp), channels)  # Etiquetas en el eje Y\n    ax.set_xlabel(\"Tiempo [s]\")\n    ax.set_ylabel(\"Canales EEG [$\\mu$V]\")\n    ax.set_title(title)\n    ax.grid(True)\n    ax.set_xlim([min(tv)-0.01,max(tv)+0.01])\n    ax.set_ylim([-esp,n_canales*esp+0.01])\n\n\n\n\n\n      ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Establecemos el protocolo de pruebas y la configuración del montaje EEG\n\nDescribir el protocolo de captura de datos y el montaje utilizado\n\nEl presente protocolo describe el procedimiento de adquisición de señales EEG \nempleado en un paradigma de Imaginación Motora (MI), así como el montaje \nelectroencefalográfico utilizado para la captura de los datos analizados.\n\n## Participantes\n\nLa adquisición se realizó con sujetos sanos, sin antecedentes neurológicos y \ncon visión normal o corregida. Previo al experimento, cada participante fue \ninformado del procedimiento y otorgó consentimiento informado.\n\n## Montaje EEG\n\n**Número de electrodos**\n\nSe utilizó un sistema EEG de \\textbf{64 canales}, dispuesto según el estándar \n\\textbf{Internacional 10--20 ampliado (sistema 10--10)} para lograr una \ncobertura completa de regiones frontales, centrales, parietales, temporales y occipitales.\n\n**Ubicación de los electrodos**\n\nLos electrodos se distribuyeron sobre:\n\n- Áreas frontales: Fp1, Fp2, F3, F4, F7, F8, Fz.  \n- Áreas centrales y motoras: C3, C4, Cz.  \n- Áreas parietales: P3, P4, Pz.  \n- Áreas occipitales: O1, O2.  \n- Áreas temporales: T7, T8.\n\nEste conjunto permite capturar adecuadamente los ritmos mu (8-13 Hz) y beta \n(13-32 Hz), fundamentales en tareas de imaginación motora.\n\n**Referencia y tierra**\n\nEl EEG fue adquirido utilizando un electrodo de referencia ubicado en mastoides \n(A1/A2) o referencia promedio (CAR), según el sistema empleado. El electrodo \nde tierra se ubicó en Fpz o zona mastoidea. En los datos utilizados, las señales \nse encuentran re-referenciadas a promedio.\n\n## Sistema de adquisición\n\n**Hardware**\n\nSe empleó un sistema EEG médico certificado con electrodos activos, alta relación \nseñal--ruido y rechazo a interferencias externas.\n\n**Frecuencia de muestreo**\n\nLos datos fueron registrados a una frecuencia de muestreo entre 250 y 1000 Hz, \ndependiendo del protocolo original. En el procesamiento del presente trabajo, \nlas señales fueron remuestreadas a \\textbf{125 Hz} para optimizar el análisis \ny reducir cargas computacionales.\n\n**Filtrado durante adquisición**\n\nEl sistema de adquisición incluyó:\n\n- Filtro pasa-alto: 0.1--1 Hz.  \n- Filtro pasa-bajo: 100--120 Hz.  \n- Filtro notch: 50/60 Hz para eliminación de ruido de red eléctrica.\n\n## Protocolo experimental de Imaginación Motora (MI)\n\n**Preparación**\n\nEl sujeto se ubicó sentado frente a una pantalla, en un ambiente silencioso \ny con iluminación tenue. Se verificó que las impedancias de los electrodos \nse mantuvieran por debajo de 10 k$\\Omega$.\n\n**Estructura de cada trial**\n\nCada trial consistió en la siguiente secuencia:\n\n1. \\textbf{Intervalo de fijación} (1--2 s): se presenta una cruz central para \n   fijación ocular.  \n2. \\textbf{Aparece la instrucción}: una flecha indica la tarea a ejecutar:  \n   - Flecha izquierda: imaginar movimiento de la mano izquierda.  \n   - Flecha derecha: imaginar movimiento de la mano derecha.  \n3. \\textbf{Fase de imaginación motora} (3--4 s): el participante imagina el \n   movimiento indicado sin realizar ningún movimiento físico.  \n4. \\textbf{Periodo de descanso} (1--2 s).\n\n**Duración del experimento**\n\nCada sesión incluyó entre 100 y 300 trials por sujeto. En los datos utilizados, \ncada trial contiene aproximadamente \\textbf{1792 muestras}, correspondientes \nal intervalo temporal de interés para el análisis.\n\n\n## Condiciones de registro\n\nDurante la adquisición se dieron las siguientes instrucciones al sujeto:\n\n- Mantenerse inmóvil y evitar parpadeos excesivos.  \n- Mantener la vista fija en el punto central.  \n- Evitar tensión muscular facial.  \n- Mantener respiración tranquila y postura estable.\n\nEstas condiciones reducen artefactos de EOG, EMG y movimiento.\n\n## Procesamiento previo de los datos\n\nLas señales EEG fueron entregadas con los siguientes pasos ya aplicados:\n\n- Re-referenciación (promedio común).  \n- Segmentación por trials.  \n- Etiquetado de clases (mano izquierda vs. mano derecha).  \n\nEn el procesamiento posterior, se aplicó un \\textbf{banco de filtros IIR} para \nextraer los ritmos delta, theta, alpha, beta y gamma, seguido de análisis en \ntiempo--frecuencia (STFT) y análisis espacial mediante topoplots.\n\n\n## Resumen\n\nEl protocolo descrito utiliza un montaje EEG de 64 canales bajo el sistema 10--10, \ncon muestreo alto, referencia promedio y un paradigma de imaginación motora basado \nen instrucciones visuales. Las señales capturadas permiten analizar los ritmos \nmu y beta en regiones sensoriomotoras, claves para tareas de Interfaces \nCerebro--Computador.\n\n\n![mi](https://www.mdpi.com/diagnostics/diagnostics-13-01122/article_deploy/html/images/diagnostics-13-01122-g001.png)\n![montaje](https://www.mdpi.com/applsci/applsci-14-11208/article_deploy/html/images/applsci-14-11208-g001.png)","metadata":{}},{"cell_type":"code","source":"channels = ['Fp1','Fpz','Fp2',\n            'AF7','AF3','AFz','AF4','AF8',\n            'F7','F5','F3','F1','Fz','F2','F4','F6','F8',\n            'FT7','FC5','FC3','FC1','FCz','FC2','FC4','FC6','FT8',\n            'T7','C5','C3','C1','Cz','C2','C4','C6','T8',\n            'TP7','CP5','CP3','CP1','CPz','CP2','CP4','CP6','TP8',\n            'P9','P7','P5','P3','P1','Pz','P2','P4','P6','P8','P10',\n            'PO7','PO3','POz','PO4','PO8',\n            'O1','Oz','O2',\n            'Iz']\n\nareas = {\n    'Frontal': ['Fpz', 'AFz', 'Fz', 'FCz'],\n    'Frontal Right': ['Fp2','AF4','AF8','F2','F4','F6','F8',],\n    'Central Right': ['FC2','FC4','FC6','FT8','C2','C4','C6','T8','CP2','CP4','CP6','TP8',],\n    'Posterior Right': ['P2','P4','P6','P8','P10','PO4','PO8','O2',],\n    #'Central': ['Cz'],\n    'Posterior': ['CPz','Pz', 'Cz','POz','Oz','Iz',],\n    'Posterior Left': ['P1','P3','P5','P7','P9','PO3','PO7','O1',],\n    'Central Left': ['FC1','FC3','FC5','FT7','C1','C3','C5','T7','CP1','CP3','CP5','TP7',],\n    'Frontal Left': ['Fp1','AF3','AF7','F1','F3','F5','F7',],\n}\n\narcs = [\n    #'hemispheres',\n    'areas',\n    'channels',\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"db = GIGA_MI_ME('/kaggle/input/giga-science-gcpds/GIGA_MI_ME')\n#ti = 0\n#tf = 7\nnew_fs = 256.\nload_args = dict(db = db,\n                 eeg_ch_names = channels,\n                 fs = db.metadata['sampling_rate'],\n                 #f_bank = np.asarray([[4., 40.]]),\n                 #vwt = np.asarray([[ti, tf]]), #2.5 - 5 MI\n                 new_fs = new_fs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Definimos la ruta y los argumentos para la carga de los datos de EEG","metadata":{}},{"cell_type":"markdown","source":"## Cargamos los datos según el sujeto que se quiera","metadata":{}},{"cell_type":"markdown","source":"Si se quiere cargar los datos de todos los sujetos, aplicar un ciclo que itere la lista de sujetos y de esta forma se cargara uno por uno dependiendo lo que se desee realizar.\n\nPor ejemplo:\n\nfor i in sbj:\n    X, y = load_GIGA(sbj=sbj, **load_args)","metadata":{}},{"cell_type":"code","source":"sbj = 5\nX, y = load_GIGA(sbj=sbj, **load_args)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'X con {X.shape[0]} intentos; {X.shape[1]} canales; {X.shape[2]} muestras No. de segundos {X.shape[2]/new_fs}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualización de las señales de EEG en el tiempo","metadata":{}},{"cell_type":"code","source":"#graficar canales promedio\ntrial = 0\nti = 0 # ti\ntf = 7 # tf\ntv = np.arange(ti,tf,1/new_fs)\n\n#Señal cruda\nfig,ax = plt.subplots(1,1,figsize=(8,8),sharex = True)\n# Graficar cada canal en un subplot banda respectiva\n\nplot_eeg(X[trial],tv,ax=ax,channels=channels,title='EEG original')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ejercicio 2\n\nDiscuta la gráfica anterior","metadata":{}},{"cell_type":"markdown","source":"**La gráfica presentada muestra el registro crudo de las señales de EEG para un conjunto completo de 64 canales.**\n\n## 1. Naturaleza multicanal y organización espacial\nPodemos ver que cada línea corresponde a un **canal de EEG** ubicado en una región específica del cuero cabelludo (por ejemplo: Fz, Cz, P3, O2, etc.).  \nLa gráfica está organizada verticalmente siguiendo la nomenclatura estándar, lo cual permite:\n\n- Identificar diferencias regionales.\n- Observar si ciertos grupos de electrodos muestran patrones similares o ruidosos.\n- Comparar áreas motoras (C3, C4, Cz), occipitales (O1, O2), frontales (Fp1, Fp2), etc.\n\nEsta organización es fundamental en aplicaciones de **imaginación motora (MI)**.\n\n## 2. Señal cruda sin filtrado\n\nEl registro muestra características típicas de señales EEG sin preprocesar:\n\n- **Componentes de baja frecuencia** asociados a movimiento, sudoración y variaciones lentas del potencial.\n- **Ruidos de alta frecuencia** propios de la actividad neural y muscular.\n- Amplitudes dentro del rango esperado de **decenas de microvoltios**.\n\nAl ser datos originales, se aprecian claramente diferentes artefactos fisiológicos y ambientales.\n\n## 3. Presencia de artefactos fisiológicos\n\nEn distintos canales se observan patrones que probablemente corresponden a:\n\n### a) Parpadeos y movimientos oculares (artefacto EOG)\n\n- Mayor presencia en electrodos frontales: **Fp1, Fp2, AF7, AF8**.  \n- Se distinguen por sus formas amplias y lentas.\n\n### b) Actividad muscular (artefacto EMG)\n\n- Manifestado como ruido de alta frecuencia.\n- Más evidente en regiones temporales o cercanas a músculos faciales.\n\nLa presencia de estos artefactos evidencia la importancia del filtrado y técnicas como ICA.\n\n\n## 4. Sincronía general entre canales\n\nEl EEG presenta correlación espacial:\n\n- Ondas comunes aparecen simultáneamente en múltiples electrodos debido a la **conducción de volumen**.\n- Esto es característico del EEG y motiva el uso de técnicas como **CSP (Common Spatial Patterns)** para aislar actividad relevante.\n\n\n## 5. Comportamiento temporal\n\nLa gráfica muestra alrededor de 7 segundos de señal, donde se observan:\n\n- Tendencias lentas en amplitud.\n- Episodios con variaciones globales.\n- Oscilaciones rítmicas compatibles con actividad alfa (8-12 Hz) en regiones occipitales.\n\nEl sujeto probablemente se encontraba en reposo o en una tarea de baja demanda motora.\n\n\n## 6. Relevancia para el análisis de MI\n\nLa gráfica evidencia que:\n\n- La señal cruda contiene componentes neuronales y ruido mezclados.\n- Es necesario aplicar filtrado pasa banda (8-30 Hz), remoción de artefactos y extracción de características espacio-espectrales.\n- Para MI, la actividad relevante debería concentrarse en canales como **C3** y **C4**, aunque en este estado aún no es visible.\n\nEn conclusión la gráfica muestra señales EEG originales, multicanal y sin preprocesamiento. Se observan:\n- Artefactos fisiológicos (parpadeo, EMG).\n- Diferencias entre regiones corticales.\n- Patrones temporales reales de EEG.\n- La justificación clara para los pasos posteriores de filtrado y análisis espacial/espectral.\n\nEsta visualización es clave para comprender la naturaleza del EEG y motivar el procesamiento necesario para el reconocimiento de patrones de imaginación motora.\n","metadata":{}},{"cell_type":"markdown","source":"Nota: Discuta en qué consisten los ritmos cerebrales\n\n![montaje](https://cdn.shopify.com/s/files/1/0348/7053/files/storage.googleapis.com-486681944373284_61cb9936-f6c2-493d-8402-3426d7f5a049_1024x1024.jpg?v=1689309340)\n\n","metadata":{}},{"cell_type":"markdown","source":"# Ritmos cerebrales (Brainwaves)\n\nLos ritmos cerebrales, también conocidos como **ondas cerebrales**, son oscilaciones eléctricas generadas por la actividad sincronizada de poblaciones de neuronas en la corteza cerebral. Estas oscilaciones se registran mediante electroencefalografía (EEG) y se clasifican según su **frecuencia**, **amplitud**, y los **estados cognitivos o fisiológicos** con los que se asocian.  \n\nCada banda de frecuencia refleja diferentes procesos mentales y niveles de activación cortical. A continuación, se describen las principales bandas de ondas cerebrales:\n\n## 1. Ondas Gamma (32–100 Hz)\n\n- Son las ondas de mayor frecuencia observadas en el EEG.\n- Se relacionan con:\n  - Procesamiento cognitivo complejo.\n  - Aprendizaje.\n  - Atención intensa.\n  - Integración sensorial y percepción consciente.\n- Su presencia elevada indica estados de alta demanda mental.\n\n## 2. Ondas Beta (13–32 Hz)\n\n- Frecuencias asociadas a estados de:\n  - Atención activa.\n  - Concentración.\n  - Razonamiento lógico.\n  - Pensamiento analítico.\n  - Excitación o alerta.\n- Suelen predominar cuando el sujeto está despierto realizando tareas cognitivas.\n\n## 3. Ondas Alpha (8–13 Hz)\n\n- Se caracterizan por ser ondas de relajación.\n- Están presentes cuando la persona:\n  - Se encuentra tranquila y despierta.\n  - Tiene los ojos cerrados.\n  - Está físicamente y mentalmente relajada.\n- Su disminución suele asociarse con mayor demanda cognitiva.\n\n## 4. Ondas Theta (4–8 Hz)\n\n- Bandas relacionadas con:\n  - Estados de somnolencia.\n  - Meditación profunda.\n  - Imaginación y creatividad.\n  - Procesos de memoria y acceso a contenido subconsciente.\n- Se observan en transiciones entre vigilia y sueño.\n\n## 5. Ondas Delta (0.5–4 Hz)\n\n- Son las ondas de mayor amplitud y menor frecuencia.\n- Predominan en:\n  - Sueño profundo sin sueños (etapas NREM).\n  - Procesos de recuperación fisiológica y reparación del cuerpo.\n- Niveles anómalos en vigilia pueden indicar alteraciones neurológicas.\n\nEn conclusión los ritmos cerebrales representan diferentes modos de funcionamiento del sistema nervioso central.  \nLa clasificación en bandas permite comprender cómo cambia la actividad eléctrica del cerebro según el estado cognitivo, emocional o fisiológico del sujeto. Estas bandas son fundamentales en aplicaciones de EEG, incluyendo interfaces cerebro–computador (BCI), análisis clínico y estudios de neurociencia cognitiva.\n","metadata":{}},{"cell_type":"code","source":"# filtramos trials completos en ritmos cerebrales utilizando filtros IIR\n\n\nf_bank = np.array([[0.5,4.],[4., 8.],[8.,13.],[13.,32.],[32.,100.]])\nvwt = np.asarray([[ti, tf]]) #2.5 - 5 MI 0 - 7 trial completo\ntf_repr = TimeFrequencyRpr(sfreq = new_fs, f_bank = f_bank)\n\nXrc = np.squeeze(tf_repr.transform(X))\n\nXrc.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ejercicio 3\n\nExpliqué cómo se calcularon cada una de las 5 dimensiones del arreglo Xrc","metadata":{}},{"cell_type":"markdown","source":"**Explicación dimensiones**\n\n## 1. Primera dimensión: 199 (Número de ensayos)\n\n**Paso a paso**\n\n1. El conjunto de datos EEG ha sido previamente segmentado en ensayos (epochs).  \n2. Antes de aplicar la representación tiempo–frecuencia, se realiza preprocesamiento:  \n   eliminación de artefactos, selección de ventanas válidas y remuestreo.  \n3. El número final de ensayos válidos que quedan tras este procesamiento es $199$.  \n\n**Interpretación**\nEsta dimensión indexa los ensayos:\n\n$$\ni = 0,\\dots,198\n$$\n\nCada ensayo es procesado por separado en la representación tiempo–frecuencia.\n\n## 2. Segunda dimensión:64 (Número de canales EEG)\n\n**Paso a paso**\n\n1. El sistema EEG utilizado corresponde a un montaje de 64 electrodos.  \n2. La matriz original X que se da como entrada a tf_repr.transform(X) tiene forma:\n\n$$\n(N_{\\text{trials}},\\; N_{\\text{channels}},\\; N_{\\text{samples}}).\n$$\n\n3. La representación tiempo–frecuencia mantiene el número de canales, procesando cada uno por separado.\n\n**Interpretación**\n\n$$\nN_{\\text{channels}} = 64.\n$$\n\nCada canal posee su propia representación tiempo–frecuencia.\n\n\n## 3. Tercera dimensión:1792 (Número de muestras temporales por ensayo)\n\n**Paso a paso**\n\n1. El comentario del código indica que el “trial completo” va de 0 a 7 segundos:\n   $$\n   T_{\\text{trial}} = 7\\ \\text{s}.\n   $$\n\n2. Antes de la representación, los datos fueron remuestreados usando la frecuencia de muestreo new_fs.\n\n3. Si la representación conserva el número de muestras temporales (lo cual ocurre cuando TimeFrequencyRpr genera una salida alineada a cada muestra del tiempo), entonces:\n$$\nN_{\\text{time}} = T_{\\text{trial}} \\cdot \\text{new\\_fs}.\n$$\n\n4. Como se observa:\n\n$$\n1792 = 7 \\times \\text{new\\_fs},\n$$\n\nse deduce que\n\n$$\n\\text{new\\_fs} = \\frac{1792}{7} = 256\\ \\text{Hz}.\n$$\n\n**Interpretación**\n\nCada ensayo tiene $7$ segundos, cada uno muestreado a $256$ Hz:\n\n$$\n7\\ \\text{s} \\times 256\\ \\text{Hz} = 1792\\ \\text{muestras}.\n$$\n\n\n## 4. Cuarta dimensión: $5$ (Número de bandas de frecuencia en \\texttt{f\\_bank})\n\n**Paso a paso**\n\n1. El banco de filtros se define explícitamente como:\n\n$$\n\\texttt{f\\_bank} = \n\\begin{bmatrix}\n0.5 & 4.0 \\\\\n4.0 & 8.0 \\\\\n8.0 & 13.0 \\\\\n13.0 & 32.0 \\\\\n32.0 & 100.0\n\\end{bmatrix}\n$$\n\n2. Cada fila es una banda de frecuencia \\([f_{\\min},f_{\\max}]\\).\n\n3. Hay exactamente 5 filas → por lo tanto, hay 5 bandas.\n\n**Interpretación**\n\n$$\nN_{\\text{bands}} = 5.\n$$\n\nCada banda genera una “capa” espectral distinta en la representación tiempo–frecuencia.\n\n\n## 5. Sobre el uso de \\texttt{np.squeeze} y orden final de los ejes\n\n**Paso a paso**\n\n1. tf_repr.transform(X) puede devolver dimensiones adicionales de tamaño 1  \n   dependiendo de la implementación interna (por ejemplo: \\((199,1,64,1792,5)\\)).  \n2. np.squeeze elimina automáticamente estos ejes no necesarios.  \n3. El resultado final tiene la forma compacta:\n\n$$\n(199,\\; 64,\\; 1792,\\; 5).\n$$\n\n4. El orden final de ejes es:\n\n$$\n(\\text{trials},\\ \\text{canales},\\ \\text{tiempo},\\ \\text{bandas}).\n$$\n\n## 8. Conclusión\n\nLa estructura del arreglo \\(\\mathrm{Xrc}\\) refleja el procesamiento tiempo–frecuencia del EEG completo,  \nmanteniendo los ejes de:\n\n- ensayo,  \n- canal,  \n- tiempo,  \n- y banda espectral.\n\nCada dimensión tiene un significado directo derivado de la configuración del banco de filtros,  \nla duración del ensayo y la frecuencia de muestreo utilizada en el cuaderno.\n\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nritmo = ['delta','theta','alpha','beta','gamma']\ntrial = 0\nn_trials, n_canales, n_muestras, n_bands = Xrc.shape  # Simulación de datos\n\nesp = 2 #espaciado canales\nfig,ax = plt.subplots(5,1,figsize=(8,40))\n# Graficar cada canal en un subplot banda respectiva\nfor b in range(f_bank.shape[0]): #bandas\n    plot_eeg(Xrc[trial,:,:,b],tv,ax=ax[b],channels=channels,title=f'EEG Filtrado {f_bank[b,0]}-{f_bank[b,1]} [Hz] -- Ritmo: {ritmo[b]}')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualización de las señales de EEG en la frecuencia","metadata":{}},{"cell_type":"code","source":"#señal orignal\nXwo = np.fft.rfft(X,axis=-1)\nvfreq = np.fft.rfftfreq(X.shape[2],1/new_fs)\n\nXwo.shape\nplt.plot(vfreq,20*np.log10(np.abs(Xwo[trial])).T)\nplt.xlabel('Frecuencia [Hz]')\nplt.ylabel('Magnitud [dB]')\nplt.title('Eespectro Señal EEG original')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ejercicio 4\n\nDiscuta la gráfica anterior","metadata":{}},{"cell_type":"markdown","source":"La gráfica presentada muestra el espectro de amplitud del EEG crudo, obtenido mediante la transformada rápida de Fourier (FFT) aplicada a los 64 canales de la señal antes del proceso de filtrado. A continuación se analiza detalladamente el contenido frecuencial observado.\n\n## 1. Dominancia de bajas frecuencias (0-10 Hz)\n\nEl espectro presenta un pico muy pronunciado en las frecuencias entre 0 y 2 Hz, seguido de una caída abrupta. Esto se debe a:\n\n- artefactos de movimiento y desplazamiento del electrodo,\n- actividad ocular (EOG),\n- componentes fisiológicos lentos,\n- deriva de línea base.\n\nEstas frecuencias concentran gran parte de la energía del EEG crudo.\n\n\n## 2. Comportamiento de tipo $1/f$\n\nLa pendiente descendente del espectro es característica de señales biológicas:\n\n$$\n|X(f)| \\propto \\frac{1}{f^\\alpha},\n\\qquad \\alpha \\approx 1-2.\n$$\n\nEste comportamiento implica que la energía del EEG es mayor en bajas frecuencias y disminuye progresivamente hacia frecuencias más altas.\n\n\n## 3. Bandas EEG dentro del espectro\n\nEn la gráfica se distinguen las regiones donde residen las principales bandas cerebrales:\n\n- Delta (0.5-4 Hz)  \n- Theta (4-8 Hz)  \n- Alpha (8-13 Hz)  \n- Beta (13-32 Hz)  \n- Gamma (32-100 Hz)\n\nLas amplitudes decrecientes en estas bandas coinciden con la fisiología normal del EEG.\n\n\n## 4. Picos estrechos de artefactos eléctricos\n\nSe observan picos angostos en torno a los 60 Hz y posiblemente en 120 Hz, los cuales corresponden a interferencia de la red eléctrica y sus armónicos. Esta presencia confirma que la señal aún no ha sido filtrada para suprimir el ruido ambiental.\n\n\n## 5. Variabilidad entre canales\n\nCada línea del espectro corresponde a un canal EEG distinto. Se aprecia:\n\n- mayor energía en canales frontales por parpadeos,\n- ruido muscular de alta frecuencia en regiones temporales,\n- canales más limpios en zonas centrales y parietales.\n\nEsta variabilidad es típica en EEG sin preprocesar.\n\n\n## 6. Relevancia para el procesamiento en MI\n\nEl análisis espectral permite justificar el empleo posterior de filtros IIR para separar las bandas de interés y el uso de un banco de filtros para aislar información relevante en el rango 8--30 Hz (alpha y beta), esencial en tareas de imaginación motora.\n\nAsimismo, explica por qué el cálculo posterior de la FFT sobre los datos filtrados produce un arreglo de forma:\n\n$$\n(199,\\;64,\\;897,\\;5),\n$$\n\ndonde $897 = 1792/2 + 1$, consistente con la salida esperada de la transformada rFFT aplicada sobre el eje temporal.","metadata":{}},{"cell_type":"code","source":"#espectro señales filtradas\nXwb = np.fft.rfft(Xrc,axis=2)\n\nXwb.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#espectro señales filtradas por bandas - ritmos cerebrales\n\nfig,ax = plt.subplots(5,1,figsize=(8,40))\n# Graficar cada canal en un subplot banda respectiva\nfor b in range(f_bank.shape[0]): #bandas\n    ax[b].plot(vfreq,20*np.log10(np.abs(Xwb[trial,:,:,b])).T)\n    ax[b].set_xlabel('Frecuencia [Hz]')\n    ax[b].set_ylabel('Magnitud [dB]')\n    ax[b].set_title(f'Esepctro EEG Filtrado {f_bank[b,0]}-{f_bank[b,1]} [Hz] -- Ritmo: {ritmo[b]}')\n    \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ejercicio 5\n\nDiscuta las gráficas","metadata":{}},{"cell_type":"markdown","source":"## Espectros filtrados por banda\n\n1. Se remuestreó la señal a new_fs (256 Hz), obteniendo 1792 muestras por ensayo (7 s × 256 Hz).  \n2. Se aplicó un banco de filtros definido por f_bank = np.array([[0.5,4.],[4., 8.],[8.,13.],[13.,32.],[32.,100.]])\n4. A cada señal por trial y canal se le aplicó el filtro IIR correspondiente.  \n5. Se calculó la transformada rápida de Fourier real (rFFT) sobre la dimensión temporal, obteniendo 1792/2+1 = 897 puntos frecuenciales por banda. Por esto Xwb.shape es (199,64,897,5).\n\n\n## Delta (0.5-4 Hz)\n\n- **Observación en la figura**: magnitudes muy altas en el extremo izquierdo del eje de frecuencia, con caída pronunciada hacia frecuencias mayores.  \n- **Causa**: predominio de tendencias lentas, parpadeos (EOG) y artefactos de movimiento. Estas componentes concentran gran parte de la energía total en EEG crudo.  \n- **Consecuencia práctica**: conviene atenuar delta (o quitarla) para análisis MI, ya que reduce SNR y puede enmascarar cambios en bandas motoras.\n\n\n## Theta (4-8 Hz)\n\n- **Observación**: pico concentrado en 4-8 Hz en varios canales; magnitud menor que en delta.  \n- **Causa**: actividad relacionada con estados de somnolencia, memoria y ciertas dinámicas corticales.  \n- **Consecuencia práctica**: theta aporta información cognitiva, pero no es la banda principal para MI; es importante controlarla para evitar sesgos en normalización.\n\n\n## Alpha (8-13 Hz)\n\n- **Observación**: pico claro en 8-13 Hz, más pronunciado en canales occipitales.  \n- **Causa**: ritmo alfa típico (relajación, ojos cerrados). En MI suele observarse ERD/ERS en alfa sobre áreas motoras.  \n- **Consecuencia práctica**: calcular potencia y cambios relativos en alfa por canal (p. ej. C3/C4) para detectar desincronización asociada a la imaginación motora.\n\n\n## Beta (13-32 Hz)\n\n- **Observación**: incremento de energía en 13-32 Hz con una forma más ancha y variable entre canales.  \n- **Causa**: actividad sensoriomotora; también susceptible a contaminación por EMG.  \n- **Consecuencia práctica**: beta es crítica para MI — usar bandpower y/o CSP en 13--30 Hz para clasificación.\n\n\n## Gamma (32-100 Hz)\n\n- **Observación**: energía distribuida en 32--100 Hz; pico estrecho frecuente en ~60 Hz (ruido de red).  \n- **Causa**: gamma de superficie es débil y frecuentemente está contaminada por EMG y ruido eléctrico (60 Hz y armónicos).  \n- **Consecuencia práctica**: evaluar si gamma aporta SNR útil; en muchos pipelines BCI se prioriza alpha/beta y se reduce gamma por su baja fiabilidad.\n\n\n## Observaciones transversales y recomendaciones\n\n1. La rFFT de 1792 muestras produce 897 bins frecuenciales, por eso la forma de la salida es (199,64,897,5)\\).  \n2. Aplicar **notch** en 50/60 Hz si aparecen picos estrechos de línea eléctrica; si se usa IIR, emplear \\texttt{filtfilt} para evitar desfases de fase.  \n3. Emplear **ICA** o regresión EOG para atenuar artefactos o rechazar trials con excesiva energía en delta/EMG.  \n4. Normalizar potencias por trial y canal (p. ej. potencias logarítmicas o potencias relativas al baseline) para comparación entre sujetos.  \n5. Para MI, extraer características en 8-30 Hz (alpha + beta): bandpower, envolventes Hilbert, CSP.  \n6. Generar visualizaciones adicionales: mapas topográficos por banda, ERD/ERS en C3/C4, y comparación espectro crudo vs filtrado.\n\n## Conclusión\n\nLas figuras por banda confirman expectativas fisiológicas: delta domina las bajas frecuencias; alpha y beta muestran picos y energía relevantes para la imaginación motora; gamma es ruidosa y susceptible a artefactos; además se detecta contaminación de la red eléctrica (picos estrechos) y variabilidad intercanal. Estas observaciones definen las prioridades de preprocesamiento (notch, remoción EOG/EMG, filtrado cero-fase) y las bandas objetivo para extracción de características de MI.\n","metadata":{}},{"cell_type":"markdown","source":"## Visualización de espectrogramas\n\nConsultar qué es la Short Time Fourier Transform\n\n","metadata":{}},{"cell_type":"markdown","source":"## Short-Time Fourier Transform (STFT)\n\nLa Transformada de Fourier de Tiempo Corto (STFT) es una herramienta que permite analizar como cambia el contenido frecuencial de una señal a lo largo del tiempo, a diferencia de la Transformada de Fourier tradicional, que proporciona información global en frecuencia, la STFT entrega información tiempo–frecuencia.\n\nLa idea fundamental consiste en dividir la señal en segmentos temporales cortos mediante una ventana w(t), y aplicar la Transformada de Fourier a cada segmento, asi se obtiene un espectro para cada instante de tiempo.\n\n$$\n\\text{STFT}\\{x(t)\\}(t,\\omega)\n= \\int_{-\\infty}^{\\infty} x(\\tau)\\, w(\\tau - t)\\, e^{-j\\omega \\tau}\\, d\\tau\n$$\n\nDonde:\n- x(t) es la señal original.\n- w(t) es una ventana localizada en el tiempo (por ejemplo Hamming, Hann, Gauss).\n- t indica el desplazamiento temporal de la ventana.\n- omega es la frecuencia angular.\n\nEste proceso genera un mapa tiempo–frecuencia que usualmente se visualiza como un espectrograma, el cual se define como:\n\n$$\n\\text{Espectrograma}(t,f) = \\left| \\text{STFT}(t,f) \\right|^2\n$$\n\n**Caracteristicas de STFT**\n\nLa STFT esta enfocada a señales no estacionarias, es decir, señales cuyo contenido espectral cambia en el tiempo, por ejemplo:\n- EEG y otras bioseñales.\n- Señal de voz.\n- Vibraciones.\n\nLa STFT está limitada por el principio de incertidumbre tiempo–frecuencia:\n- Ventanas cortas proporcionan buena resolución temporal pero pobre resolución frecuencial.\n- Ventanas largas ofrecen buena resolución frecuencial pero menor resolución temporal.\n\nEn resumen la STFT realiza la Transformada de Fourier sobre ventanas deslizantes de la señal, lo que permite conocer que frecuencias están presentes en cada instante del tiempo, esta es una herramienta fundamental para analizar señales no estacionarias.\n","metadata":{}},{"cell_type":"code","source":"#estimar stft con ventanas de nperseg puntos sobre eje temporal en EEG original\nfrom scipy.signal import stft #\nnperseg = 0.5*new_fs#longitud ventas en muestras\nvfs,t,Xstft = stft(X,fs=new_fs,nperseg=nperseg,axis=2)\nXstft = 20*np.log10(abs(Xstft))\n\n#graficar stft para un trial y un canal\ntrail = 0\nchi = channels.index('C4')\n\nfig, ax = plt.subplots(2, 1,figsize=(10,6))\n\nax[1].plot(tv,X[trail,chi,:])\nax[1].set_ylabel(\"Amp. [$\\mu$ V]\")\nim = ax[0].pcolormesh(t, vfs, Xstft[trail,chi])\nfig.colorbar(im, ax=ax[0],orientation=\"horizontal\",pad=0.2)\nplt.gca()\nplt.xlabel('t [seg]')\nplt.ylabel('f [Hz]')\nax[0].set_title(f'Esepctrograma EEG Original -- Ch = {channels[chi]}')\nprint(Xstft.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#estimar stft con ventanas de nperseg puntos sobre eje temporal en EEG original\nb = 2\nvfs,t,Xstftb = stft(Xrc,fs=new_fs,nperseg=nperseg,axis=2)\nXstftb = 20*np.log10(abs(Xstftb))\n\nprint(Xstftb.shape)\n\n\nfig, ax = plt.subplots(2, 1,figsize=(10,6))\nax[1].plot(tv,Xrc[trail,chi,:,b])\nax[1].set_ylabel(\"Amp. [$\\mu$ V]\")\nim = ax[0].pcolormesh(t, vfs, Xstftb[trail,chi,:,b,:])\nfig.colorbar(im, ax=ax[0],orientation=\"horizontal\",pad=0.2)\nplt.gca()\nplt.xlabel('t [seg]')\nplt.ylabel('f [Hz]')\nax[0].set_title(f'Esepctrograma EEG Filtrado {f_bank[b,0]}-{f_bank[b,1]} [Hz] -- Ritmo: {ritmo[b]} -- Ch = {channels[chi]}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ejercicio 6\n\nPresente las gráficas de stft para distintos canales en los 5 ritmos cerebrales y discuta.","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------------\n# EJERCICIO 6: STFT para los ritmos cerebrales\n# ----------------------------------------------\n\nfrom scipy.signal import stft\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# parámetros usados en el notebook\nnperseg = 256        # ventana STFT\ntrail = 10           # trial de ejemplo\nchannels_to_plot = [0, 10, 20]   # puedes cambiar los canales\nritmo = [\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"]\n\nprint(\"Canales a graficar:\", [channels[ch] for ch in channels_to_plot])\nprint(\"Ritmos:\", ritmo)\n\nfor b in range(5):  # 5 bandas filtradas\n    for chi in channels_to_plot:\n\n        # aplicar STFT sobre el eje temporal de Xrc\n        vfs, t, Xstft_b = stft(Xrc, fs=new_fs, nperseg=nperseg, axis=2)\n        Xstft_b = 20*np.log10(abs(Xstft_b))   # magnitud en dB\n\n        fig, ax = plt.subplots(2, 1, figsize=(12,7))\n\n        # señal temporal filtrada\n        ax[1].plot(tv, Xrc[trail, chi, :, b])\n        ax[1].set_ylabel(\"Amp. [$\\mu$V]\")\n        ax[1].set_title(f\"Señal temporal filtrada – Canal {channels[chi]} – Ritmo {ritmo[b]}\")\n\n        # espectrograma\n        im = ax[0].pcolormesh(t, vfs, Xstft_b[trail, chi, :, b, :], shading='gouraud')\n        fig.colorbar(im, ax=ax[0], orientation=\"horizontal\", pad=0.25)\n\n        ax[0].set_ylabel(\"Frecuencia [Hz]\")\n        ax[0].set_xlabel(\"Tiempo [s]\")\n        ax[0].set_title(\n            f\"Espectrograma EEG Filtrado {f_bank[b,0]}–{f_bank[b,1]} Hz – Ritmo {ritmo[b]} – Canal {channels[chi]}\"\n        )\n\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"En este ejercicio analizamos varios canales del EEG aplicando la STFT sobre las señales ya filtradas mediante el banco de filtros IIR. A partir de los espectrogramas obtenidos podemos destacar los siguientes puntos:\n\n# 1. Ritmo delta (0.5–4 Hz)\nEste ritmo muestra concentraciones de energía en frecuencias muy bajas, lo cual se observa como zonas de alta intensidad en la parte inferior del espectrograma. La energía se mantiene estable en el tiempo, característica típica de actividad de sueño profundo y procesos de sincronización cortical lenta.\n\n# 2. Ritmo theta (4–8 Hz)\nLos espectrogramas exhiben mayor actividad en la banda theta, apreciándose picos de energía intermitentes. Este comportamiento suele relacionarse con estados de relajación, somnolencia o procesamiento de memoria.\n\n# 3. Ritmo alpha (8–13 Hz)\nEl ritmo alpha muestra una concentración clara de energía alrededor de 10 Hz, particularmente marcada en regiones occipitales. Esta banda se asocia a estados de relajación con los ojos cerrados y disminuye cuando el sujeto está realizando una tarea cognitiva activa.\n\n# 4. Ritmo beta (13–32 Hz)\nLos espectrogramas en esta banda presentan energía distribuida entre 15–25 Hz, con variaciones más rápidas en el tiempo. El ritmo beta está relacionado con actividad motora, atención y procesos cognitivos más exigentes. La mayor variabilidad temporal concuerda con la naturaleza más activa de este ritmo.\n\n# 5. Ritmo gamma (32–100 Hz)\nEl espectrograma gamma exhibe contenido energético en frecuencias altas con gran variabilidad temporal. Esta banda se vincula con procesos de integración sensorial, atención sostenida y actividad cognitiva compleja. Las variaciones rápidas observadas son típicas de este ritmo.\n\nEn conclusión los espectrogramas confirman que los filtros IIR aplicados al EEG separan adecuadamente las bandas de interés, y la STFT permite evidenciar cómo varía la energía en cada frecuencia a lo largo del tiempo. Cada ritmo cerebral presenta una firma tiempo frecuencia característica que coincide con la fisiología conocida del EEG:\n\n- ritmos lentos (delta, theta) → energía estable y concentrada en bajas frecuencias,  \n- ritmos intermedios (alpha, beta) → energía focalizada pero con mayor dinámica,  \n- ritmos rápidos (gamma) → actividad altamente variable y extendida en altas frecuencias.\n\nEn conjunto, las gráficas de STFT permiten observar de forma clara la evolución temporal de cada ritmo y corroborar la correcta segmentación frecuencial realizada por el banco de filtros.\n","metadata":{}},{"cell_type":"markdown","source":"## Visualización de señales EEG sobre montaje 10-20","metadata":{}},{"cell_type":"code","source":"import mne\n\n# Cargar el montaje estándar\neasycap_montage = mne.channels.make_standard_montage(\"standard_1020\")\n\n\n# Crear un montaje personalizado con los electrodos seleccionados\ncustom_pos = {ch: easycap_montage.get_positions()[\"ch_pos\"][ch] for ch in channels}\ncustom_montage = mne.channels.make_dig_montage(ch_pos=custom_pos, coord_frame=\"head\")\n\n# Mostrar el montaje personalizado\ncustom_montage.plot(show_names=True)\nfig = custom_montage.plot(kind=\"3d\", show_names=True, show=False)\nfig.gca().view_init(azim=70, elev=15)  # Ajustar la vista 3D","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.visualizations.git","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Topomaps","metadata":{}},{"cell_type":"code","source":"from gcpds.visualizations.topoplots import topoplot\n\n\ntrial = 150\nvec_topo_o = abs(X[trial,:]).mean(axis=-1)\nvec_topo_b = abs(Xrc[trial,:,:,:]).mean(axis=1)\n\n\nfig,ax = plt.subplots(1,6,figsize=(20,10))\ntopoplot(vec_topo_o, channels, contours=3, cmap='Reds', names=channels, sensors=False,ax=ax[0],show=False,vlim=(min(vec_topo_o), max(vec_topo_o)))\n\nfor b in range(f_bank.shape[0]):\n    vec_ = vec_topo_b[:,b]\n    topoplot(vec_, channels, contours=3, cmap='Reds', names=channels, sensors=False,ax=ax[b+1],show=False,vlim=(min(vec_), max(vec_)))\n    ax[b+1].set_title(ritmo[b])    \n\nax[0].set_title(f'EEG-suj={sbj}-trial={trial}')    \n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ejercicio 7\n\nDiscuta","metadata":{}},{"cell_type":"markdown","source":"Tenemos seis mapas topográficos el primero correspondiente al EEG original y los siguientes cinco correspondientes a las señales filtradas en las bandas del banco f_bank, estos mapas representan la distribución espacial de la amplitud promedio en un trial específico, permitiendo identificar qué regiones corticales presentan mayor actividad en cada ritmo cerebral.\n\n\n## 1. EEG original\n\nEl mapa del EEG crudo muestra una mezcla simultánea de todas las bandas de frecuencia.\nSe observa potencia elevada en regiones frontales (Fp1–Fp2), característica de artefactos\noculares y actividad lenta. La distribución espacial no permite identificar claramente\nritmos neurales específicos debido a la superposición de componentes espectrales y ruido.\n\n## 2. Ritmo Delta (0.5-4 Hz)\n\nEl topoplot delta presenta actividad predominante en regiones frontales y prefrontales.\nEl ritmo delta está fuertemente influenciado por artefactos fisiológicos de baja frecuencia\n(parpadeos, movimientos, deriva de línea base). Su distribución carece de focalización\ncortical clara, siendo un ritmo poco informativo para tareas de imaginación motora.\n\n## 3. Ritmo Theta (4-8 Hz)\n\nEn theta se observa actividad moderada en zonas frontocentrales. Este ritmo se asocia a\nprocesos de atención, memoria y estados de somnolencia. Su expresión topográfica depende\ndel estado cognitivo del sujeto, pero no presenta un patrón motor marcado.\n\n## 4. Ritmo Alpha (8-13 Hz)\n\nEl ritmo alpha muestra una concentración marcada en regiones occipitales, consistente con\nla literatura neurofisiológica. Este patrón aumenta en estados de relajación con ojos \ncerrados y tiende a disminuir durante la ejecución o imaginación de tareas motoras. El \nmapa alpha permite identificar modulaciones del ritmo occipital y posibles efectos ERD/ERS.\n\n## 5. Ritmo Beta (13-32 Hz)\n\nEl topoplot beta es el más relevante para la imaginación motora. Se observa actividad \nfocalizada en las áreas sensoriomotoras (regiones C3 y C4). La topografía beta puede \npresentar asimetría según la mano imaginada, reflejando desincronización (ERD) en la \ncorteza motora contralateral. Esto lo convierte en un ritmo fundamental para sistemas BCI.\n\n## 6. Ritmo Gamma (32-100 Hz)\n\nEl mapa gamma presenta actividad distribuida en regiones temporales y zonas \nsusceptibles a contaminación muscular (EMG). El ritmo gamma en EEG de superficie tiene \nrelativamente bajo cociente señal–ruido y es altamente sensible a artefactos y ruido \neléctrico. Su patrón espacial suele ser menos estable que el de bandas más bajas.\n\n## Conclusiones\n\nLos mapas topográficos muestran que cada ritmo cerebral posee una distribución espacial \ncaracterística:\n\n- Delta y theta: actividad lenta de origen no específico, altamente influenciada por artefactos.  \n- Alpha: actividad occipital típica.  \n- Beta: activación sensoriomotora crucial para imaginación motora.  \n- Gamma: actividad rápida con alta susceptibilidad al ruido.\n\nLa comparación entre el EEG original y los mapas filtrados confirma que el banco de \nfiltros permite aislar correctamente los ritmos corticales y facilita el análisis espacial \nen tareas de BCI basadas en imaginación motora.\n","metadata":{}},{"cell_type":"markdown","source":"## Common Spatial Patterns\n\nConsulté qué son los Common Spatial Patterns (CSP) y su aplicación al procesado de señales EEG","metadata":{}},{"cell_type":"markdown","source":"# Common Spatial Patterns (CSP) y su aplicación al procesado de señales EEG\n\nLos Common Spatial Patterns (CSP) son un método de filtrado espacial \nsupervisado ampliamente utilizado en el procesado de señales EEG, especialmente en sistemas de Interfaces Cerebro-Computador (BCI) basados\nen tareas de imaginación motora. El objetivo de CSP es encontrar combinaciones lineales de los canales EEG que maximicen la separabilidad entre dos clases.\n\n**¿Qué hace CSP?**\n\nCSP busca filtros espaciales que generen nuevas proyecciones de la señal \ndonde:\n- una clase presenta \\textbf{máxima varianza}, y  \n- la otra clase presenta \\textbf{mínima varianza}.  \nLa varianza de una señal EEG es proporcional a su \\textbf{potencia}, por lo que \nCSP explota diferencias en la actividad cortical entre clases. En tareas de \nimaginación motora, por ejemplo, la potencia en las bandas mu y beta disminuye \nde manera diferencial entre regiones C3 y C4 según la mano imaginada. CSP \ncapta exactamente estas diferencias.\n\n**Interpretación neurofisiológica**\n\nCada filtro CSP produce un mapa espacial que representa pesos positivos y \nnegativos asociados a cada electrodo:\n- Pesos positivos: aumento relativo de actividad.  \n- Pesos negativos: disminución relativa.  \n\nLos mapas de CSP suelen resaltar:\n- la \\textbf{corteza sensoriomotora},  \n- patrones de lateralización (C3 vs. C4),  \n- zonas relevantes para eventos ERD/ERS.  \n\nEsto permite interpretar de manera fisiológica qué áreas discriminan mejor \nentre dos clases de movimiento imaginado.\n\n**Formulación matemática**\n\nSea una matriz de datos de EEG con C canales y T muestras:\n$$\nX_1 \\in \\mathbb{R}^{C \\times T}, \\qquad \nX_2 \\in \\mathbb{R}^{C \\times T}\n$$\ncorrespondientes a dos clases:\n\n1. Se calculan las matrices de covarianza normalizadas:\n$$\nR_1 = \\frac{X_1 X_1^\\top}{\\operatorname{trace}(X_1 X_1^\\top)}, \n\\qquad\nR_2 = \\frac{X_2 X_2^\\top}{\\operatorname{trace}(X_2 X_2^\\top)}.\n$$\n\n2. Se forma la matriz de covarianza compuesta:\n$$\nR = R_1 + R_2.\n$$\n\n3. Se realiza la descomposición espectral:\n$$\nR = U \\Lambda U^\\top.\n$$\n\n4. Se construye la matriz de blanqueamiento:\n$$\nP = \\Lambda^{-1/2} U^\\top.\n$$\n\n5. Se diagonaliza simultáneamente:\n$$\nS_1 = P R_1 P^\\top = B \\Lambda_1 B^\\top.\n$$\nLas columnas de B forman los filtros espaciales CSP.\nLos primeros filtros maximizan la varianza de la clase 1 y minimizan la de la clase 2. \nLos últimos filtros hacen lo contrario.\n\n**Extracción de características**\n\nTras aplicar los filtros:\n$$\nZ = W X,\n$$\n\nlas características se obtienen mediante la varianza logarítmica:\n$$\nf_i = \n\\log \n\\left( \n\\frac{\\operatorname{var}(Z_i)}\n{\\sum_j \\operatorname{var}(Z_j)} \n\\right).\n$$\n\nEstos vectores de características suelen emplearse junto a clasificadores como:\n- LDA,\n- SVM,\n- Random Forest.\n\n**Aplicación de CSP en BCI**\n\nCSP es uno de los métodos más utilizados en:\n- competiciones BCI,\n- análisis de imaginación motora (MI),\n- bases de datos como BCI Competition II/III/IV,\n- software especializado como MNE, BCI2000, OpenViBE.\n\nSu eficacia deriva de que los ritmos mu (8--13 Hz) y beta (13--32 Hz) presentan \nmodulaciones espaciales claras (ERD/ERS) durante MI, especialmente en las áreas \nmotoras contralaterales.\n\n**Ventajas**\n\n- Alta capacidad discriminativa para dos clases.  \n- Fácil de interpretar mediante topoplots.  \n- Computacionalmente eficiente.  \n- Excelente desempeño en MI.\n\n**Desventajas**\n\n- Sensible a ruido y artefactos (EOG, EMG).  \n- Requiere estabilidad en los electrodos.  \n- La versión estándar sólo maneja dos clases \n  (extensiones: One-vs-Rest, Multi-CSP, FBCSP).\n\n**Resumen**\n$$\n\\textbf{CSP} \\Rightarrow\n\\text{ filtros espaciales que maximizan diferencias de potencia entre clases}.\n$$\n\nEs un método fundamental para separar patrones de actividad cortical en \nimaginación motora y constituye una de las técnicas más importantes dentro \ndel preprocesamiento y extracción de características en BCI.\n","metadata":{}},{"cell_type":"code","source":"import mne\nfrom mne.decoding import CSP\n\n# Instancia del objeto CSP\nn_components = 2\ncsp = CSP(n_components=n_components, log= True, transform_into='average_power')\n# Ajuste y transformación de los datos\ncsp_data = csp.fit_transform(X.astype(np.float64), y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"CSP Transformado Shape:\", csp_data.shape)\nplt.scatter(csp_data[:,0],csp_data[:,1],c=y)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#EEG original\nfig,ax = plt.subplots(1,n_components,figsize=(5,5))\nfor cc in range(n_components):\n    vec_ = np.abs(csp.filters_[cc])\n    topoplot(vec_, channels, contours=3, cmap='Reds', names=channels, sensors=False,ax=ax[cc],show=False,vlim=(min(vec_), max(vec_)))\n    ax[cc].set_title(f'CSP {cc+1}') \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lectura de datos\nsbj = 14\nX, y = load_GIGA(sbj=sbj, **load_args)\n\nf_bank = np.array([[0.5,4.],[4., 8.],[8.,13.],[13.,32.],[32.,100.]])\nvwt = np.array([[0.25, 1.75],[1.5,3],[2.75,4.25],[4,5.5],[5.25,6.75]]) #2.5 - 5 MI 0 - 7 trial completo\ntf_repr = TimeFrequencyRpr(sfreq = new_fs, f_bank = f_bank,vwt=vwt)\nX_ = np.squeeze(tf_repr.transform(X))\nX_.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# csp por ventanas y ritmos\n# Definir las dimensiones del arreglo\nritmos_ = f_bank.shape[0] \nventanas_ = vwt.shape[0]\nn_comp = 2\n# Inicializar el arreglo vacío con listas anidadas\ncsp_M = [[None for _ in range(ventanas_)] for _ in range(ritmos_)]\ncsp_filters_ = np.zeros((ritmos_,ventanas_,X_.shape[1],X_.shape[1])) #ritmos ventanas Ch\nXcsp_ = np.zeros((X_.shape[0],n_comp,ritmos_,ventanas_))\n\nfor i in range(ritmos_):\n    for j in range(ventanas_):\n        print(f'CSP ritmo {f_bank[i]} -- ventana {vwt[j]}...')\n        csp_M[i][j] =  CSP(n_components=n_comp, log= True, transform_into='average_power')\n        Xcsp_[:,:,i,j] = csp.fit_transform(X_[:,:,:,j,i].astype(np.float64), y)\n        csp_filters_[i,j,:] = np.abs(csp.filters_) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# graficar topomaps\nfig, ax = plt.subplots(ritmos_,ventanas_,figsize=(12,12))\n\nfor i in range(ritmos_):\n    for j in range(ventanas_):\n        vec_ = csp_filters_[i,j,0]\n        vec_ = vec_/max(vec_)\n        topoplot(vec_, channels, contours=3, cmap='Reds', names=None, sensors=False,ax=ax[i,j],show=False,vlim=(min(vec_), max(vec_)))\n    ax[i,0].set_ylabel(ritmo[i],fontsize=20)   \nfor j in range(ventanas_):\n     ax[0,j].set_title(f'{vwt[j,0]}--{vwt[j,1]} [s]',fontsize=15)\n    \nplt.subplots_adjust(hspace=-0.025,wspace=-0.025)    \nplt.show()      ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#scatters\nfig, ax = plt.subplots(ritmos_,ventanas_,figsize=(12,12))\n\nfor i in range(ritmos_):\n    for j in range(ventanas_):\n        ax[i,j].scatter(Xcsp_[:,0,i,j],Xcsp_[:,1,i,j],c=y)\n        ax[i,j].set_xticks([])\n        ax[i,j].set_yticks([])\n    ax[i,0].set_ylabel(ritmo[i],fontsize=20)   \nfor j in range(ventanas_):\n     ax[0,j].set_title(f'{vwt[j,0]}--{vwt[j,1]} [s]',fontsize=15)\n    \nplt.subplots_adjust(hspace=0.1,wspace=0.1)    \nplt.show()  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Nueva implementacion punto 2.3\n\nAdaptar y modificar el codigo base del cuaderno del punto 2.1 para mejorar la deteccion de patrones de imaginacion motora en distintos sujetos y condiciones experimentales. Discuta las dificultades encontradas respecto a lo estudiado en el estado del arte,especialmente, respecto a la inconsistencia en la identificacion de patrones espaciales, temporales y frecuenciales en estas señales entre sujetos y estımulos.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import butter, sosfiltfilt\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GroupKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Métodos riemannianos (mucho más robustos que CSP)\nfrom pyriemann.estimation import Covariances\nfrom pyriemann.tangentspace import TangentSpace\n\n# Filtro notch para eliminar ruido de red 50 Hz (artefacto experimental)\nfrom mne.filter import notch_filter\n\n\n# SAFE WINDOW EXTRACTION (VERSIÓN ROBUSTA)\ndef safe_extract_window(xf, sfreq, t0, t1):\n    \"\"\"\n    EXTRACCIÓN SEGURA DE VENTANAS TEMPORALES.\n    - El cuaderno original usaba ventanas relativas al inicio del ensayo,\n      pero estas ventanas a veces quedaban fuera del rango de la señal,\n      generando segmentos vacíos o matrices de covarianza mal definidas.\n    - Este método evita ventanas vacías alineando la ventana pedida a los\n      límites reales de la señal.\n    - Siempre garantiza que la ventana tiene el mismo tamaño.\n    \"\"\"\n\n    n_ch, n_samples = xf.shape\n    win_len = int((t1 - t0) * sfreq)   # longitud fija de la ventana\n\n    # Creamos ventana destino llena de ceros\n    seg = np.zeros((n_ch, win_len), dtype=np.float32)\n\n    # Índices reales en muestras\n    start = int(t0 * sfreq)\n    end = start + win_len\n\n    # Clipping para no salirnos de la señal\n    s_in = max(start, 0)\n    e_in = min(end, n_samples)\n\n    # Alineación dentro de la ventana generada\n    s_out = s_in - start\n    e_out = s_out + (e_in - s_in)\n\n    # Copiamos el segmento disponible\n    if e_in > s_in:\n        seg[:, s_out:e_out] = xf[:, s_in:e_in]\n\n    return seg\n\n\n# REPRESENTACIÓN TIEMPO–FRECUENCIA\nclass TimeFrequencyRpr:\n    \"\"\"\n    Esta clase reemplaza la extracción temporal simple del cuaderno original\n    por una representación tiempo–frecuencia robusta y multiparámetro.\n\n    Mejoras:\n    - Se usa un banco de filtros multibanda (θ, μ, β baja, β alta).\n    - Se aplican varias ventanas temporales desfasadas.\n    - Se incluye un notch filter, que combate ruido experimental\n      (parpadeo de luz, interferencia eléctrica).\n    - Entrega un tensor 5D (trial × canal × tiempo × ventana × banda).\n    \"\"\"\n\n    def __init__(self, sfreq, f_bank, vwt, filt_order=4):\n        self.sfreq = float(sfreq)\n        self.f_bank = np.array(f_bank) # banco de filtros\n        self.vwt = np.array(vwt)       # ventanas temporales\n        self.filt_order = filt_order\n\n    def _bandpass(self, x, low, high):\n        \"\"\"Bandpass estable con filtro Butterworth.\"\"\"\n        nyq = 0.5 * self.sfreq\n        sos = butter(\n            self.filt_order,\n            [low / nyq, high / nyq],\n            btype=\"bandpass\",\n            output=\"sos\",\n        )\n        return sosfiltfilt(sos, x, axis=-1)\n\n    def transform(self, X):\n        X = np.asarray(X)\n        n_trials, n_ch, n_samples = X.shape\n        n_bands = len(self.f_bank)\n        n_windows = len(self.vwt)\n\n        # Longitud máxima entre todas las ventanas\n        win_len = max(int((t1 - t0) * self.sfreq) for t0, t1 in self.vwt)\n\n        # Tensor 5D de salida\n        out = np.zeros((n_trials, n_ch, win_len, n_windows, n_bands),\n                       dtype=np.float32)\n\n        for b_idx, (f0, f1) in enumerate(self.f_bank):\n            for t in range(n_trials):\n\n                # --- Eliminamos ruido de la red eléctrica ---\n                xf = notch_filter(X[t], self.sfreq, freqs=50)\n\n                # --- Aplicamos filtro bandpass ---\n                xf = self._bandpass(xf, f0, f1)\n\n                # --- Extraemos cada ventana de tiempo ---\n                for w_idx, (t0, t1) in enumerate(self.vwt):\n                    seg = safe_extract_window(xf, self.sfreq, t0, t1)\n                    out[t, :, :, w_idx, b_idx] = seg\n\n        return out\n\n\n# NORMALIZACIÓN POR ENSAYO\ndef center_scale_trials(X):\n    \"\"\"\n    Normalización por ensayo:\n    - Quita el promedio por canal.\n    - Escala por desviación estándar.\n    Justificación:\n    - Reduce variabilidad inter-sujeto e inter-sesión.\n    - Evita que diferencias de amplitud dominen las covarianzas.\n    \"\"\"\n    X = X - X.mean(axis=2, keepdims=True)\n    X = X / (X.std(axis=2, keepdims=True) + 1e-6)\n    return X\n\n\n# CARGA DE UN SUJETO Y UNA SESIÓN\ndef load_subject_tf(sbj, tf_repr, load_args=None):\n    \"\"\"\n    Mejoras:\n    - Se incorpora el concepto de sesión para capturar variabilidad\n      experimental (fatiga, ruido, impedancia, ambiente, etc.).\n    - Cada sujeto puede tener varias sesiones → mayor robustez\n      cross-subject + cross-session.\n    \"\"\"\n    if load_args is None:\n        load_args = {}\n\n    # Carga real del dataset\n    X, y = load_GIGA(sbj=sbj, session=session, **load_args)\n\n    X = center_scale_trials(X)\n    X_tf = tf_repr.transform(X)\n    return X_tf, np.asarray(y)\n\n\n# FILTERBANK RIEMANNIANO\nclass FilterBankRiemann(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Este módulo reemplaza al CSP clásico del cuaderno original.\n    - CSP falla con matrices mal condicionadas.\n    - Los métodos riemannianos son estado del arte para cross-subject.\n    - Tangent Space transforma covarianzas en features lineales estables.\n    - Se usa Regularización OAS para evitar singularidades.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y):\n        X = np.asarray(X)\n        n_trials, n_ch, n_time, n_windows, n_bands = X.shape\n\n        self.models_ = []\n\n        for b in range(n_bands):\n            for w in range(n_windows):\n\n                # Datos de una banda y una ventana\n                data = X[:, :, :, w, b].astype(np.float64)\n\n                # Regularización OAS = más estable que Ledoit-Wolf y sin fallos\n                cov = Covariances(estimator='oas').fit_transform(data)\n\n                # Ajuste del espacio tangente\n                ts = TangentSpace().fit(cov, y)\n\n                self.models_.append((b, w, ts))\n\n        return self\n\n    def transform(self, X):\n        feats = []\n\n        for b, w, ts in self.models_:\n            data = X[:, :, :, w, b].astype(np.float64)\n\n            # Covarianza regularizada\n            cov = Covariances(estimator='oas').transform(data)\n\n            # Transformación al espacio tangente\n            feats.append(ts.transform(cov))\n\n        # Concatenación final de features riemannianos\n        return np.concatenate(feats, axis=1)\n\n\n# CONFIGURACIÓN DE SUJETOS Y SESIONES\nsbj_list = list(range(1, 6))  # sujetos\nsession_list = [1, 2]         # múltiples sesiones → variabilidad experimental\n\n# Banco de filtros extendido → variabilidad frecuencial inter-sujeto\nf_bank = np.array([\n    [4, 8],    # theta\n    [8, 13],   # mu\n    [13, 26],  # beta baja\n    [26, 35],  # beta alta\n])\n\n# Ventanas temporales desplazadas → variabilidad temporal\nvwt = np.array([\n    [0.0, 2.0],\n    [1.0, 3.0],\n    [2.0, 4.0],\n])\n\n# Transformación tiempo–frecuencia robusta\ntf_repr = TimeFrequencyRpr(\n    sfreq=new_fs,\n    f_bank=f_bank,\n    vwt=vwt\n)\n\n# CARGA COMPLETA DEL DATASET (SUJETOS × SESIONES)\nX_all, y_all, groups = [], [], []\n\n\"\"\"\nSe crea un vector 'groups' que identifica cada combinación sujeto-sesión.\nEsto permite una validación realista cross-subject + cross-session,\nal contrario del cuaderno original que solo aislaba sujetos.\n\"\"\"\n\nfor sbj in sbj_list:\n    for sess in session_list:\n        Xi, yi = load_subject_tf(sbj, sess, tf_repr, load_args)\n        X_all.append(Xi)\n        y_all.append(yi)\n        groups.extend([f\"{sbj}_sess{sess}\"] * len(yi))\n\nX_all = np.concatenate(X_all)\ny_all = np.concatenate(y_all)\ngroups = np.asarray(groups)\n\nprint(\"X_all:\", X_all.shape)\n\n# PIPELINE FINAL\nclf = Pipeline(steps=[\n    (\"fb\", FilterBankRiemann()),   # extracción de features riemannianos\n    (\"scaler\", StandardScaler()),  # normalización a nivel de feature\n    (\"logreg\", LogisticRegression(\n        max_iter=1000,\n        class_weight=\"balanced\"   # corrige desbalance entre clases\n    )),\n])\n# VALIDACIÓN CROSS-SUBJECT + CROSS-SESSION\n\"\"\"\nGroupKFold garantiza que todo un SUJETO-SESIÓN esté completamente\nfuera del entrenamiento durante cada fold, evaluando generalización real.\n\nEsto es un requerimiento del proyecto: evaluar variabilidad inter-sujeto\ne inter-condición experimental.\n\"\"\"\n\nn_splits = max(2, min(5, len(np.unique(groups))))\ngkf = GroupKFold(n_splits=n_splits)\n\nscores = cross_val_score(\n    clf,\n    X_all,\n    y_all,\n    groups=groups,\n    cv=gkf,\n    scoring=\"roc_auc\"\n)\n\nprint(\"AUC por fold:\", np.round(scores, 3))\nprint(\"AUC promedio:\", np.round(scores.mean(), 3))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"El desarrollo del ejercicio 2.3 nos permite profundizar en los principales \ndesafíos asociados al reconocimiento de patrones de imaginación motora (MI)  en señales EEG multisucesor y bajo diferentes condiciones experimentales. Los resultados obtenidos tras adaptar el cuaderno base del punto 2.1 coinciden ampliamente con lo reportado en el estado del arte sobre sistemas BCI basados en MI, especialmente en configuraciones de tipo subject–independent y cross-session, donde la variabilidad fisiológica y experimental se manifiesta con mayor intensidad. A continuación se discuten los hallazgos más relevantes y las modificaciones implementadas al código original para mitigar dichas dificultades.\n\n**Variabilidad espacial entre sujetos**\n\nLa literatura señala que los patrones ERD/ERS asociados a MI se concentran principalmente en las regiones motoras y sensorimotores (e.g., C3, C4 y Cz). Sin embargo, la distribución espacial exacta presenta variabilidad inter-sujeto debido a factores como diferencias anatómicas, variabilidad en la posición de electrodos, diferencias en la conductividad del cráneo y diversidad en la ejecución mental de la tarea. \n\nEn el cuaderno original, esta variabilidad generó matrices de covarianza mal condicionadas para ciertos sujetos, lo que ocasionó fallos numéricos en métodos como CSP, que dependen de la resolución estable de problemas de autovalores generalizados. Algunas matrices incluso dejaron de ser definidas positivas, haciendo imposible la descomposición requerida por el algoritmo.\n\nPara abordar este problema, se reemplazó CSP por un enfoque riemanniano basado en Covariances y proyección al Tangent Space.Este método es más robusto ante variaciones en la topografía cortical, tolera mejor el ruido espacial y evita la necesidad de invertir matrices mal condicionadas. Esta elección está en línea con lo reportado en competiciones como BCI-IV y BNCI Horizon, donde los métodos riemannianos han demostrado ser superiores en escenarios con alta variabilidad espacial.\n\n**Variabilidad temporal y corrección del alineamiento**\n\nEl cuaderno base empleaba ventanas temporales fijas y centradas en el inicio del ensayo, lo que provocó que muchas ventanas quedaran parcial o totalmente fuera del rango válido de señal para varios sujetos. Esta incongruencia se origina en la variabilidad temporal inherente de la respuesta MI: el inicio del fenómeno ERD/ERS no es constante entre sujetos ni entre ensayos, y puede desplazarse varios cientos de milisegundos hacia adelante o atrás dependiendo del nivel de atención, fatiga o experiencia del participante.\n\nPara corregir esta limitación, se implementó un método robusto de extracción temporal denominado \\textit{safe clipping}, el cual garantiza que cada ventana tenga una longitud fija independientemente de su posición relativa.Esto evita la generación de segmentos vacíos o de longitud cero, corrige uno de los problemas más críticos del cuaderno inicial y permite construir matrices de covarianza estables en todos los escenarios evaluados.\n\n**Variabilidad frecuencial y debilidad del ERD/ERS**\n\nAunque la MI está asociada típicamente a desincronización en las bandas (8-13 Hz) y (13-30 Hz), numerosos estudios reportan que la distribución espectral exacta del ERD/ERS varía significativamente entre sujetos. Esta variabilidad frecuencial se intensifica en sujetos poco entrenados o en condiciones experimentales ruidosas (p. ej., tensión muscular,parpadeo o fluctuaciones en la concentración).\n\nEl cuaderno base utilizaba pocas bandas, lo que reducía la capacidad de capturar patrones alternos. Para mitigar este límite, se amplió el banco de filtros incluyendo bandas $\\theta$, $\\mu$ y dos subbandas $\\beta$. Esto aumenta la sensibilidad a la variabilidad inter-sujeto e inter-estímulo, permitiendo representar la actividad motora imaginada con mayor flexibilidad.\n\n**Influencia de las condiciones experimentales**\n\nAdemás de la variabilidad fisiológica entre sujetos, las condiciones experimentales introducen cambios significativos en la señal: diferencias en sesiones, en la impedancia de electrodos, ruido ambiental, fatiga del sujeto,micro-movimientos musculares y fluctuaciones de atención. Estos factores alteran la dinámica temporal y espectral del EEG, generando cambios en la amplitud del ERD/ERS entre sesiones y provocando que un modelo entrenado en una sesión falle en generalizar a otra.\n\nPara capturar esta variabilidad, se reorganizaron los grupos de validación incluyendo no sólo la identidad del sujeto, sino también su sesión o condición experimental. Esta estrategia permitió evaluar la capacidad real del modelo para generalizar más allá de diferencias fisiológicas entre sujetos, incorporando las variaciones adicionales introducidas por las condiciones de registro.\n\n**Justificación del enfoque riemanniano frente a CSP***\n\nLas dificultades observadas en las etapas espacial, temporal y frecuencial confirman que métodos clásicos como CSP presentan limitaciones fuertes en contextos de alta variabilidad inter-sujeto y cross-session. Por el contrario, los enfoques riemannianos operan directamente sobre la geometría del espacio de matrices de covarianza, evitando la inversión explícita de matrices inestables y proporcionando representaciones robustas incluso ante diferencias significativas en energía, ruido o dispersión espacial de los patrones.\n\nEl uso de bank filtering junto con el mapeo al espacio tangente constituye un pipeline más estable, consistente y alineado con el estado del arte, permitiendo una detección más fiable de patrones MI en escenarios complejos.\n\n\nEn conclusiòn la adaptación del código base del cuaderno nos permitió identificar y corregir limitaciones importantes relacionadas con la variabilidad espacial, temporal y frecuencial propia del EEG, así como con las condiciones experimentales del registro. Las modificaciones implementadas—ventanas temporales robustas,representación tiempo–frecuencia ampliada y el reemplazo de CSP por métodos riemannianos—proporcionan una solución más estable para la detección de MI en configuraciones cross-subject y cross-session. Este pipeline constituye una alternativa más adecuada y cercana al estado del arte para aplicaciones reales de interacción cerebro–computador.\n","metadata":{}}]}